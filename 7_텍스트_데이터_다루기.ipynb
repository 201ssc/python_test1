{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7. 텍스트 데이터 다루기.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMCkFJbUXAxxSeKF/8srcU2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/201ssc/python_test1/blob/main/7_%ED%85%8D%EC%8A%A4%ED%8A%B8_%EB%8D%B0%EC%9D%B4%ED%84%B0_%EB%8B%A4%EB%A3%A8%EA%B8%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4장에선 데이터 속성을 나타내는 두 가지 특성에 관해 이야기 했다. 정량적인 연속형 특성과 고정된 값인 범주형 특성. 많은 애플리케이션에서 사용하는 세번째 유형의 데이터가 있는데, 그건 텍스트이다. 회사에선 고객 메시지가 불만사항인지 문의사항인지 구분해야 할 때가 많다 . 메시지의 제목이나 내용으로 고객의 의도를 자동으로 파악해서 적절한 부서로 전달하거나, 완전 자동으로 응답할 수도 있다. \n",
        "\n",
        "\\\n",
        "텍스트 데이터는 주로 글자가 연결된 문자열로 표현된다. 텍스트 데이터의 길이는 서로 같은 경우가 거의 없다. 이런 특성은 이제까지 본 수치형 특성과 매우 다르므로 머신러닝 알고리즘에 적용하기 전에 전처리를 해야한다.\n",
        "\n",
        "### 7.1 문자열 데이터 타입\n",
        "텍스트 데이터를 처리하는 과정에 들어가기 전에, 자주 나타나는 몇 가지 텍스트 데이터에 대해 알아보자. 문자열 데이터는 4가지 종류가 있다.\n",
        "- 범주형 데이터\n",
        "- 범주에 의미를 연결시킬 수 있는 임의의 문자열\n",
        "- 구조화된 문자열 데이터\n",
        "- 텍스트 데이터\n",
        "\n",
        "\\\n",
        "**범주형 데이터**는 고정된 목록으로 구성된다. 예를 들어 선호하는 색깔(빨주노초파남보) 이 경우 데이터셋에 7개의 값중 하나가 들어가면 범주형 변수로 인코딩된다. 그리고 데이터셋에 고유값을 찾아 이 값들이 얼마나 자주 나타나는지 히스토그램을 그려볼 수 있다. 만약 설문이 절반정도 진행됐는데 누군가 '검정'을 '검점'으로 쓴 오타를 발견해서 설문을 수정했다면, 이 데이터셋에는 같은 의미를 나타내는 검정과 검점이 모두 들어 있으므로 두 값을 하나로 합쳐야한다.\n",
        "\n",
        "만약 선호하는 색을 묻기위해 드롭다운 메뉴가 이닌, 텍스트 필드를 제공했다고 보자. 어떤 사람은 철자를 틀리거나 회색이나 쥐색처럼 다르게 쓸수도 있다. 또 더 확실하고 구체적인 암청색이라고 쓸 수도 있다. 또한 '캘리포니아 해변 암초의 색' 처럼 사람들이 자기만의 이름을 붙이는 경우 자동 맵핑이 힘들다. 텍스트 필드로 받은 이런 응답에는 **범주에 의미를 연결시킬 수 있는 임의의 문자열**에 해당하는데, 이런 변수를 인코딩하려면 보편적인 값을 선택하든, 애플리케이션에 맞게 이런 응답을 포용할 수 있는 범주를 정의하는 게 최선이다. 여러 색의 경우라면 '여러가지색' 인코딩할 수 없는건 '그 외'라고 해야한다. 그렇기 때문에 범주형 변수로 받을 수 있는 것은 직접 입력받지 말라고 권한다.\n",
        "\n",
        "**구조**는 미리 정의된 범주에 속하지는 않지만 주소나 장소, 사람이름, 날짜. 전화번호, 식별번호처럼 일정한 구조를 가지는 것이다.\n",
        "\n",
        "**텍스트 테이터**는 트윗, 채팅, 호텔리뷰, 셰익스피어 작품 ,위키백과 문서, 구텐베르크 프로텍트, 50,000권의 전자책 등이 여기에 속한다.텍스트 분석에서는 데이터셋을 말뭉치라 하고, 하나의 텍스트를 의미하는 각 데이터 포인트를 문서라 한다. 이런 용어는 텍스트 데이터를 주로 다루는 정보검색과 자연어처리 공동체에서 유래했다."
      ],
      "metadata": {
        "id": "_2DjJcNBj9QR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "#if 'google.colab' in sys.modules and not os.path.isdir('mglearn'):\n",
        "    # 사이킷런 최신 버전을 설치합니다.\n",
        "!pip install -q --upgrade scikit-learn\n",
        "# mglearn을 다운받고 압축을 풉니다.\n",
        "!wget -q -O mglearn.tar.gz https://bit.ly/mglearn-tar-gz\n",
        "!tar -xzf mglearn.tar.gz\n",
        "# 나눔 폰트를 설치합니다.\n",
        "!sudo apt-get -qq -y install fonts-nanum\n",
        "import matplotlib.font_manager as fm\n",
        "fm._rebuild()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiBJrFIs7Zla",
        "outputId": "a6a4f9e2-5b9f-4ada-a506-849bb4ddfc4c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package fonts-nanum.\n",
            "(Reading database ... 155676 files and directories currently installed.)\n",
            "Preparing to unpack .../fonts-nanum_20170925-1_all.deb ...\n",
            "Unpacking fonts-nanum (20170925-1) ...\n",
            "Setting up fonts-nanum (20170925-1) ...\n",
            "Processing triggers for fontconfig (2.12.6-0ubuntu2) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from preamble import *\n",
        "import matplotlib\n",
        "\n",
        "# 나눔 폰트를 사용합니다.\n",
        "matplotlib.rc('font', family='NanumBarunGothic')\n",
        "matplotlib.rcParams['axes.unicode_minus'] = False"
      ],
      "metadata": {
        "id": "xRfyONAC7cLa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import mglearn\n",
        "import matplotlib\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "from sklearn.preprocessing import QuantileTransformer, StandardScaler, PowerTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import seaborn as sns\n",
        "sns.set(font_scale=3)\n",
        "sns.set(rc = {'figure.figsize':(10,6)})\n",
        "\n",
        "from pandas.core.common import random_state"
      ],
      "metadata": {
        "id": "xMIJhUuFFIi3"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.2 예제 앱: 영화 리뷰 감성 분석"
      ],
      "metadata": {
        "id": "8RKwlw-fj9S0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path\n",
        "\n",
        "if not os.path.isfile('data/aclImdb_v1.tar.gz'):\n",
        "  !wget -q http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz -P data\n",
        "  !tar -xzf data/aclImdb_v1.tar.gz -C data\n",
        "    "
      ],
      "metadata": {
        "id": "tmviHGivj9VU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !은 셸(shell) 명령을 실행해주는 IPython의 매직 명령어입니다.\n",
        "# tree 명령이 없다면 find ./data -type d 명령을 사용해 하위 폴더의 목록을 \n",
        "# 볼 수 있습니다. 윈도에서는 !tree data/aclImdb 와 같이 사용하세요.\n",
        "#!tree -dL 2 data/aclImdb\n",
        "!find ./data -type d"
      ],
      "metadata": {
        "id": "I068nrZwj9Xs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a3638fa-9e23-4527-82c4-9bce775c53f6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./data\n",
            "./data/aclImdb\n",
            "./data/aclImdb/test\n",
            "./data/aclImdb/test/neg\n",
            "./data/aclImdb/test/pos\n",
            "./data/aclImdb/train\n",
            "./data/aclImdb/train/unsup\n",
            "./data/aclImdb/train/neg\n",
            "./data/aclImdb/train/pos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r data/aclImdb/train/unsup  ## 긍정 부정 레이블이 없는 데이터 삭제"
      ],
      "metadata": {
        "id": "Q9f2vKsIgNIj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_files\n",
        "\n",
        "reviews_train = load_files(\"data/aclImdb/train/\")\n",
        "# 텍스트와 레이블을 포함하고 있는 Bunch 오브젝트를 반환합니다.\n",
        "text_train, y_train = reviews_train.data, reviews_train.target\n",
        "print(\"text_train의 타입:\", type(text_train))\n",
        "print(\"text_train의 길이:\", len(text_train))\n",
        "print(\"text_train[6]:\\n\", text_train[6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbkfHN3jgRFc",
        "outputId": "495e6d72-c49b-4877-f9a2-18560b89ddee"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text_train의 타입: <class 'list'>\n",
            "text_train의 길이: 25000\n",
            "text_train[6]:\n",
            " b\"This movie has a special way of telling the story, at first i found it rather odd as it jumped through time and I had no idea whats happening.<br /><br />Anyway the story line was although simple, but still very real and touching. You met someone the first time, you fell in love completely, but broke up at last and promoted a deadly agony. Who hasn't go through this? but we will never forget this kind of pain in our life. <br /><br />I would say i am rather touched as two actor has shown great performance in showing the love between the characters. I just wish that the story could be a happy ending.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_train = [doc.replace(b\"<br />\", b\" \") for doc in text_train]"
      ],
      "metadata": {
        "id": "XIOYV-ACgRH-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"클래스별 샘플 수 (훈련 데이터):\", np.bincount(y_train))"
      ],
      "metadata": {
        "id": "I4ef4os1gRKu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14c43677-8c9d-468c-8c2d-c749f80dd09a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "클래스별 샘플 수 (훈련 데이터): [12500 12500]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_train[7]"
      ],
      "metadata": {
        "id": "tOYP63FdgRNu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47b8cd03-e045-4484-ef38-3932efd175a5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b\"the single worst film i've ever seen in a theater. i saw this film at the austin film festival in 2004, and it blew my mind that this film was accepted to a festival. it was an interesting premise, and seemed like it could go somewhere, but just fell apart every time it tried to do anything. first of all, if you're going to do a musical, find someone with musical talent. the music consisted of cheesy piano playing that sounded like they were playing it on a stereo in the room they were filming. the lyrics were terribly written, and when they weren't obvious rhymes, they were groan-inducing rhymes that showed how far they were stretching to try to make this movie work. and you'd think you'd find people who could sing when making a musical, right? not in this case. luckily they were half talking/half singing in rhyme most of the time, but when they did sing it made me cringe. especially when they attempted to sing in harmony. and that just addresses the music. some of the acting was pretty good, but a lot of the dialog was terrible, as well as most of the scenes. they obviously didn't have enough coverage on the scenes, or they just had a bad editor, because they consistently jumped the line and used terrible choices while cutting the film. at least the director was willing to admit that no one wanted the script until they added the hook of making it a musical. i hope the investors make sure someone can write music before making the same mistake again.\""
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_test = load_files(\"data/aclImdb/test/\")\n",
        "text_test, y_test = reviews_test.data, reviews_test.target\n",
        "print('테스트 테이터의 문서 수:', len(text_test))\n",
        "print('클래스별 샘플 수 (테스트 데이터):', np.bincount(y_test))\n",
        "text_test = [doc.replace(b'<br />', b' ') for doc in text_test]"
      ],
      "metadata": {
        "id": "2DlfVg8UgRP1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe0c616c-cfb5-479a-e218-f553134a8868"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "테스트 테이터의 문서 수: 25000\n",
            "클래스별 샘플 수 (테스트 데이터): [12500 12500]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "풀려는 문제는 다음과 같다. 리뷰 하나가 주어졌을때, 이 리뷰의 텍스트 내용을 보고 '양성'인지 '음성'인지 구분하는 것이다. 이는 전형적 인진 분류 문제이다. 그러나 텍스트 데이터는 머신러닝 모델이 다룰 수 있는 형태가 아니다. 그래서 텍스트의 문자열 표현을 머신러닝 알고리즘에 적용할 수 있도록 수치 표현으로 바꿔야 한다."
      ],
      "metadata": {
        "id": "0ixMr9IlgRSI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.3 텍스트 데이터를 BOW로 표현하기\n",
        "BOW는 널리쓰이는 방법이다. 쓰면 장, 문단, 문장, 서식 같은 입력 테스트의 구조 대부분을 잃고 각 단어가 이 말뭉치에 있는 텍스트에 얼마나 많이 나타나는지만 헤아린다. 구조와 상관없이 단어의 출현횟수만 세기때문에 텍스트를 담는 bag으로 생각할 수 있다.\n",
        "전체 말뭉치에 대핸 bow 표현을 계산하려면 다음 세 단계를 거친다.\n",
        "\n",
        "1. < 토큰화 > 각 문서를 문서에 포함된 단어(토큰)로 나눈다. 예를 들어 공백이나 구두점 등을 기준으로 나눈다.\n",
        "2. < 어휘 사전 구축 > 모든 문서에 나타난 모든 단어의 어휘를 모으고 번호를 매긴다.(알파벳순서)\n",
        "3. < 인코딩 > 어휘 사전의 단어가 문서마다 몇 번이나 나타나는지를 헤아린다.\n"
      ],
      "metadata": {
        "id": "OyTXgj77gRWf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.3.1 샘플 데이터에 BOW 적용\n"
      ],
      "metadata": {
        "id": "MSavH2KK1Nkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bards_words =[\"The fool doth think he is wise,\",\n",
        "              \"but the wise man knows himself to be a fool\"]"
      ],
      "metadata": {
        "id": "0VH63OsF1NnB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vect = CountVectorizer() \n",
        "vect.fit(bards_words) # 토큰화"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jJWk7Ax1Nph",
        "outputId": "cf6be71a-eba7-4c61-b87f-c9b662a1d3c7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer()"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"어휘 사전의 크기:\", len(vect.vocabulary_))\n",
        "print(\"어휘 사전의 내용:\\n\", vect.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZ3rAf7u1NsJ",
        "outputId": "d2a105b1-a8fb-4157-e448-2d6c1860bae5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "어휘 사전의 크기: 13\n",
            "어휘 사전의 내용:\n",
            " {'the': 9, 'fool': 3, 'doth': 2, 'think': 10, 'he': 4, 'is': 6, 'wise': 12, 'but': 1, 'man': 8, 'knows': 7, 'himself': 5, 'to': 11, 'be': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bag_of_words = vect.transform(bards_words)\n",
        "print(\"BOW:\", repr(bag_of_words))\n",
        "# 2 x 13 행렬"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OQd4teF1Nuo",
        "outputId": "1cf022d3-9957-464a-cbe1-14301080a363"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BOW: <2x13 sparse matrix of type '<class 'numpy.int64'>'\n",
            "\twith 16 stored elements in Compressed Sparse Row format>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('BOW 의 밀집 표현 : \\n', bag_of_words.toarray())"
      ],
      "metadata": {
        "id": "aYadiGwe1NxM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d48140e-1139-410e-ad9e-effa77d9fd81"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BOW 의 밀집 표현 : \n",
            " [[0 0 1 1 1 0 1 0 0 1 1 0 1]\n",
            " [1 1 0 1 0 1 0 1 1 1 0 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.3.2 영화 리뷰에 대한 BOW\n",
        " 이제 영화 리뷰에 대한 감성 분석을 적용해 보겠다. 앞의 IMDb 리뷰의 훈련 데이터와 테스트 데이터를 읽어서 작업할 문자열 리스트로 바꿔보겠다. (text_train과 text_test)\n",
        " "
      ],
      "metadata": {
        "id": "uWLWJDsp-OvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vect = CountVectorizer().fit(text_train)\n",
        "X_train = vect.transform(text_train)\n",
        "print('X_train : \\n', repr(X_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7E-CZ0z-PgZ",
        "outputId": "c80b2ea8-432b-402f-9531-5daaada364ba"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train : \n",
            " <25000x74849 sparse matrix of type '<class 'numpy.int64'>'\n",
            "\twith 3431196 stored elements in Compressed Sparse Row format>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "크기 25,000 x 74,849로 어휘 사전은 단어를 74,849 개 담고있다. 이 데이터는 Scipy 희소 행렬로 저장되어 있다. 어휘 사전을 조금 더 자세히 살펴보면 CountVectorizer 객체의 get_feature_names_out 메서드는 각 특성에 해당하는 단어를 리스트로 반환한다."
      ],
      "metadata": {
        "id": "WunbtafM-Pm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = vect.get_feature_names_out()\n",
        "print('특성 개수:', len(feature_names))\n",
        "print('처음 20개 특성:\\n', feature_names[:20])\n",
        "print('20010에서 20030까지 특성:\\n', feature_names[20010:20030])\n",
        "print('매 2000번째 특성:\\n', feature_names[::2000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNx6Hp9h-Pst",
        "outputId": "cc6b3c8c-ac89-4768-e91a-9913c0f2552e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "특성 개수: 74849\n",
            "처음 20개 특성:\n",
            " ['00' '000' '0000000000001' '00001' '00015' '000s' '001' '003830' '006'\n",
            " '007' '0079' '0080' '0083' '0093638' '00am' '00pm' '00s' '01' '01pm' '02']\n",
            "20010에서 20030까지 특성:\n",
            " ['dratted' 'draub' 'draught' 'draughts' 'draughtswoman' 'draw' 'drawback'\n",
            " 'drawbacks' 'drawer' 'drawers' 'drawing' 'drawings' 'drawl' 'drawled'\n",
            " 'drawling' 'drawn' 'draws' 'draza' 'dre' 'drea']\n",
            "매 2000번째 특성:\n",
            " ['00' 'aesir' 'aquarian' 'barking' 'blustering' 'bête' 'chicanery'\n",
            " 'condensing' 'cunning' 'detox' 'draper' 'enshrined' 'favorit' 'freezer'\n",
            " 'goldman' 'hasan' 'huitieme' 'intelligible' 'kantrowitz' 'lawful' 'maars'\n",
            " 'megalunged' 'mostey' 'norrland' 'padilla' 'pincher' 'promisingly'\n",
            " 'receptionist' 'rivals' 'schnaas' 'shunning' 'sparse' 'subset'\n",
            " 'temptations' 'treatises' 'unproven' 'walkman' 'xylophonist']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "의미 없는 단어들이 보인다. 하지만 의미없어 보이는 것중 다시 의미있는 걸 선별하는 건 쉽지 않아 보인다. 또한 'draught', 'draughts' 같은 단수 복수형 구분일 뿐 의미가 매우 비슷한 것을 다르게 구분하는 것도 바람직하지 않을 것이다. \n",
        "\n",
        "특성 추출 방법을 개선하기 전에, 분류기를 만들어 성능 수치를 확인해보자. 일반적으로 희소행렬의 고차원 데이터셋에는 LogisticRegression 같은 선형 모델의 성능이 가장 뛰어나다.\n",
        "교차검증을 통해 log 모델의 성능을 평가해보자."
      ],
      "metadata": {
        "id": "f8oN4vup-PzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "scores = cross_val_score(LogisticRegression(max_iter=1000), X_train, y_train, n_jobs=-1)\n",
        "print('교차 검증 평균 점수: {:.2f}'.format(np.mean(scores)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMLi4Kcl-QIR",
        "outputId": "81d5f0d8-def3-47b2-da72-d88ad8dc905a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "교차 검증 평균 점수: 0.88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# log 에는 규제 매개변수 C 가 있으므로 그리드 서치를 사용해서 조정해보겠다.\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
        "grid = GridSearchCV(LogisticRegression(max_iter=5000), param_grid, n_jobs=-1)\n",
        "grid.fit(X_train, y_train)\n",
        "print(\"최상의 교차 검증 점수: {:.2f}\".format(grid.best_score_))\n",
        "print(\"최적의 매개변수: \", grid.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEyV60xxEZ5s",
        "outputId": "104bf0dc-1f86-4d1b-f7c0-4858ffd12e2e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최상의 교차 검증 점수: 0.89\n",
            "최적의 매개변수:  {'C': 0.1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# C=0.1 에서 교차 검증 점수 89%를 얻었다. 이 매개변수를 사용해서 테스트 셋 일반화 성능을 확인해보자\n",
        "X_test = vect.transform(text_test)\n",
        "print(\"테스트 점수: {:.2f}\".format(grid.score(X_test, y_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8d59XiCEaD1",
        "outputId": "1f41067d-c6a3-42aa-9e93-a00aae040526"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "테스트 점수: 0.88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "l4NeuuIHEaKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6tgaORSAEaQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fPZc4-QhEaVK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}